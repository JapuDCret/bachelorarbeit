% \chapter{Beispielhafte Integration}
	
\section{Anforderungen}
\input{content/04_erstellung-poc/anforderungen}
	
\section{Vorstellung der Demoanwendung}

	\textit{In diesem Abschnitt soll die Demoanwendung vorgestellt werden, anhand dessen das Proof-of-Concept erstellt wird. Damit das Proof-of-Concept erstellt werden kann, muss die Demoanwendung die zuvor beschriebenen Probleme aufweisen, hierbei sollen die Probleme möglichst realitätsnah sein und nicht frei erfunden.}
	
\newpage
	
\section{Konzept}
	
	\subsection{Datenverarbeitung}

	Auf Basis der zuvor vorgestellten Methoden und Praktiken wird nun eine sinnvolle Kombination konzeptioniert, die als Ziel hat, die Nachvollziehbarkeit nachhaltig zu erhöhen. Es werden die Disziplinen Datenerhebung, -auswertung und -visualisierung unterschieden und nacheinander beschrieben. Danach und darauf aufbauend wird eine grobe Architektur vorgestellt, die diese Ansätze in ein Gesamtbild bringt.
		
	\subsubsection{Erhebung}
		
	Im Standardfall erhalten Betreiber und Entwickler, bis auf die Kommunikation mit Partnersystemen, keine Informationen von einer SPA. Deswegen sollen zunächst Loginformationen des Frontends erhoben und an ein weiteres System gesendet werden. Hierbei ist eine Unterscheidung sinnvoll, welche Logmeldungen tatsächlich gesendet werden sollen (bspw. anhand der Log Kritikalität). Diese Unterscheidung sollte konfigurativ änderbar sein. Dieser Datenstrom wird erfahrungsgemäß unregelmäßig aber doch schon sehr häufig mit Daten befüllt, um eine gute Nachvollziehbarkeit zu erreichen.
		
	Neben den Loginformationen sollten nicht gefangene Fehler und optional gefangene Fehler an ein weiteres System weitergeleitet werden. Dabei sollen alle relevanten und zugreifbaren Informationen mitgesendet werden.
		
	Eine Datenerhebung wie beim Real-User-Monitoring, wo jede Benutzerinteraktion aufgezeichnet wird, um bspw. Klickpfade zu optimieren oder um festzustellen wie lange ein Nutzer sich auf einer Seite aufhielt. Jedoch ist ein Session-Replay Mechanismus enorm hilfreich und gewünscht, welcher ebenso jede Benutzerinteraktion aufzeichnen muss. Damit nicht zu viele Daten erhoben werden und damit nicht jede Nutzersitzung mitgeschnitten wird, soll die Aufzeichnung erst nach Einwilligung und Aktivierung seitens des Nutzers erfolgen oder bei speziellen Umgebungen automatisch (bspw. einer Staging-Umgebung). Weiterhin könnten die Loginformationen nach dieser Einwilligung auch feingranularer übertragen werden, bspw. ohne Einwilligung würden Logs der Kritikalität INFO und höher übertragen werden und mit Einwilligung auch Logs der Kritikalität DEBUG und höher.
		
	Des Weiteren soll ein Tracing der Kommunikation zwischen Frontend und Partnersystemen eingesetzt werden. Hierbei könnten auch wichtige Verarbeitungsmethoden im Tracing erfasst werden, dies wird jedoch hier nicht definiert und ist Teil der eigentlichen Implementierung. Es soll wenn möglich auf den neuen Standard OpenTelemetry (OTel) aufgesetzt werden. Hierdurch wird das Erheben von Tracing- und Metrikdaten standardisiert und zukünftig auch für Logdaten möglich. Auf Basis von OTel sollen beispielhaft Metriken erfasst werden, um das Anwendungsverhalten nachzuhalten und zu überwachen. Durch diese Metriken können Aspekte eines Application Performance Monitorings erfüllt werden.
		
	Alle gesendeten Datensätze sollen möglichst mit Metadaten angereichert werden, die einerseits den Nutzer, die Umgebung und die Anwendung identifizieren. Diese umfassen u. A.: Zeitstempel, Session-Id, User-Agent, IP, Hostsystem, Version.
		
	\subsubsection{Auswertung}
		
	Bei der Auswertung der Daten soll hauptsächlich auf bestehende Technologien aufgebaut werden, wie z.B. die Technologien, die in \autoref{sec:werkzeuge-und-technologien} evaluiert wurden. Das heißt im Konzept kann nicht im Detail darauf eingegangen werden, wie diese Daten tatsächlich verarbeitet werden und dies ist auch nicht direkt von Relevanz. Eine Beschreibung folgt im \autoref{sec:technologie-stack} sowie beim Einsatz von den Technologien selbst.
		
	Lediglich bei der Vorverarbeitung des Tracings im Client kann nun bereits eine Aussage getroffen werden. Wird hierbei nämlich, wie gewünscht, auf OTel gesetzt, dann erfolgt eine Vorverarbeitung von den Komponenten von OTel selbst. Dabei werden u. A. die Spankontexte fürs Tracing und die Beziehung zwischen den Spans gepflegt.
	
	\subsubsection{Visualisierung}
		
	Wie bei der Auswertung, wird auch die Visualisierung stark abhängig von den eingesetzten Technologien sein. Nichtsdestotrotz können bereits hier gewünschte Ansichten/Funktionen definiert werden:
	
	\begin{itemize}
		\item Die Logdaten sollen abrufbar sein und filterbar sein.
		\item Fehlerinformationen sollen gesondert der Logdaten dargestellt werden.
		\begin{itemize}
			\item Fehler sollen gruppiert werden, um gleiche Fehlerbilder zusammenzufassen.
			\item Zu den Fehlerbildern sollen übergreifende Informationen dargestellt werden.
			\item Es lassen sich auch einzelne Fehler einer Fehlergruppe anzeigen.
		\end{itemize}
		\item Für einen ausgewählten Trace soll ein Trace-Gantt-Diagramm dargestellt werden (vgl. \autoref{sec:tracing})
		\item Die beispielhaften Metriken soll visuell dargestellt werden.
		\item Die Daten zum Session-Replay sollen, wenn vorhanden, visuell dargestellt werden, sodass die Interaktionen des Nutzers nachzuvollziehen sind.
	\end{itemize}
	
	\subsection{Architektur}

	Auf Basis der zuvor definierten Grundkonzepte zur Datenverarbeitung, wird nun eine grobe Architektur konzipiert. Im Allgemeinen sollen die Funktionsbereiche Log Management, Error Monitoring, Application Monitoring, Tracing sowie Session-Replay erfüllt werden. Im Client soll es für jeden Funktionsbereich eine eigene Komponente geben, die die jeweiligen Daten erfasst und an das entsprechende Partnersystem weiterleitet. Lediglich die Erfassung von Metriken und Tracing soll auf Basis von OpenTelemetry erfolgen und daher dieselben Komponenten verwenden. Dieser Aufbau lässt sich auf der linken Seite der \autoref{fig:grobe-architektur} betrachten.
	
	Es wurde nach \autoref{anf:3100} versucht die Anzahl der Partnersysteme gering zu halten. Für die Datenverarbeitung, -analyse und -visualisierung von Logs, Fehlern und Metriken soll ein einzelnes System zuständig sein. Denn für die Bewältigung dieser verschiedenen Kategorien sind ähnliche Disziplinen notwendig, wodurch ein einzelnes System ausreichen sollte. Grundlegende Funktionen dieses Systems belaufen sich auf die Langzeitspeicherung, eine performante Suche und die Visualisierung in Graphen.
	
	Für Tracing wird ein zweites System benötigt, hauptsächlich weil in der Evaluation kein Werkzeug identifiziert werden konnte, welches neben den zuvor genannten 3 Datenkategorien auch Tracingdaten gut unterstützt. Tracingdaten sind zudem anders, dadurch dass sie einen hohen Datendurchsatz und -menge aufweisen.
	
	Ein drittes System soll die Funktionalität rund um Session-Replay übernehmen. Hierbei liegt auch der Grund darin, dass kein Werkzeug identifiziert werden konnte, welches neben Session-Replay auch andere Disziplinen erfüllen kann. Weiterhin sind, wie beim Tracing, die Eigenschaften der Daten anders, denn hier werden nahezu konstant Daten versendet, um alle Benutzerinteraktionen und das Anwendungsverhalten nachstellen zu können.
	
\begin{figure}[H]
	\centering
	\includegraphics[width=0.65\linewidth]{img/04_erstellung-poc/konzept-simple.png}
	\caption{Grobe Architektur}
	\label{fig:grobe-architektur}
\end{figure}

Wie in der Datenerfassung erwähnt, werden die einzelnen Datentypen unterschiedlich erhoben und besitzen somit auch andere Eigenschaften. Wie IBM bei Big Data definiert \cite{ZikopoulosUnderstandingBigData}, lassen sich auch hier die Eigenschaften Volume, Velocity und Variety identifizieren. Der Aspekt Volume ist weniger präsent, denn die Datenmengen sind nicht vergleichbar mit echten Big-Data-Anwendungen. Genau ist dies nicht prognostizierbar, aber in der Evaluierung des Stands der Technik, ließ sich ein Datendurchsatz von 1 MB/min feststellen - somit stellt dies im Frontend keine Herausforderung dar, jedoch in den verarbeitenden Systemen kann dies natürlich durch eine große Menge an Frontends multipliziert werden, was jedoch nicht im Fokus der Arbeit steht.

Eine Variety der Daten, also Unterschiedlichkeit der Datenstruktur, ist definitiv vorhanden und entspringt den verschiedenen Funktionsgebieten. Auch innerhalb derselben Datenströme kann eine Variety identifiziert werden, denn bspw. sind Logmeldungen sehr individuell, sie folgen meist nicht streng einem Format und enthalten unterschiedliche Mengen an Informationen.

Der Aspekt des Velocity ist zudem auch sehr wichtig und eine Visualisierung dessen für das vorhergehende Konzept findet sich in \autoref{fig:grobe-architektur-datendurchsatz}.
	
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{img/04_erstellung-poc/konzept-datendurchsatz.png}
	\caption{Grobe Architektur mit hervorgehobenem Datendurchsatz}
	\label{fig:grobe-architektur-datendurchsatz}
\end{figure}

\pagebreak

	\subsection{Technologie-Stack}
	\label{sec:technologie-stack}

	Im Frontend sind für den Logger und den Error-Handler keine speziellen Technologien zu verwenden, es soll somit auf die bestehende Demoanwendung aufgesetzt werden. Da die Demoanwendung in Angular geschrieben ist, soll ein ErrorHandler implementiert werden, welcher die Information dann weiterleitet. Die Tracing- und Metrikdaten werden mit OpenTelemetry JavaScript-Komponenten erfasst und an ein Partnersystem gesendet. Für die Session-Replay-Daten wird jedoch auf eine proprietäre Komponente gesetzt, denn bei der Evaluierung konnte keine quelloffene Datenerhebung identifiziert werden.
	
	Als Log- und Monitoringsystem soll Splunk zum Einsatz kommen. Nach der Evaluation von Splunk lassen sich hiermit gut die drei Datenkategorien Logs, Metriken und Fehler speichern, analysieren und visualisieren. Alternativen wären u. A. Dynatrace und New Relic, basieren jedoch auf einer proprietären Datenerhebung. Diese bieten eher ausgefertigte Dashboards und Graphen, mit zu weniger Flexibilität zur Datenvisualisierung. Honeycomb wäre zudem auch eine Alternative, eignet sich aber weniger für strikte Logdaten. Unter anderem werden die Daten bei Honeycomb nur für 60 Tage gespeichert. Somit blieb Splunk als einziger Kandidat, der den gewünschten Rahmen erfüllt. Es sollten jedoch auch andere Werkzeuge Splunk ersetzen können, bspw. der Elastic Stack könnte hierfür auch geeignet sein, dieser wurde jedoch nicht zuvor evaluiert.
	
	Für das Partnersystem, welches Session-Replay übernimmt, wurde nur ein Werkzeug evaluiert, nämlich LogRocket. Da diese Art von Software als sehr zielführend empfunden wurde, wird LogRocket somit in der Lösung verwendet. Jedoch ist anzumerken, dass ähnlich wie bei Splunk, LogRocket nicht zwingend vorgeschrieben ist, sondern gegen ein äquivalentes Werkzeug austauschbar ist (vgl. \autoref{sec:weitere-werkzeuge}). LogRocket kann jedoch auch überzeugen, denn es bietet Funktionen, um datenschutzkritische Aspekte zu beachten aber auch um die Performanz der eigentlichen Anwendung nicht zu stark einzuschränken. So wird LogRockets eigentliche Logik erst nach dem Seitenladen dynamisch nachgeladen und die Hauptarbeit findet generell in Worker-Threads statt, nicht im Hauptthread \cite{LogRocketPerformance}. Des Weiteren schränkt LogRocket die Aufnahmequalität ein, wenn eine unzureichende Bandbreite bemerkt wird.

\begin{wrapfigure}[9]{r}{0.4\textwidth}
\centering
\includegraphics[width=\linewidth]{img/04_erstellung-poc/ComparisonDataSerializationFormats_figure4.png}
\caption{Serialisierte Datengröße je Format \cite{ComparisonDataSerializationFormats}}
\label{fig:serialized-data-size}
\end{wrapfigure}
	
	Allgemein werden bei LogRocket zudem Daten nicht im JSON-Format kommuniziert, sondern mit Google Protobufs (protocol buffers) \cite{GoogleProtobufs}, welche effizienter sind (siehe \autoref{fig:serialized-data-size}). Die Verwendung von Protobufs könnte ggf. auch interessant sein bei der Datenübertragung der anderen Datenkategorien, jedoch wird sich an dieser Stelle nicht darauf festgelegt. Sollte eine hohe Übertragungseffizienz tatsächlich von Nöten sein, so werden Protobufs ggf. näher betrachtet.

\begin{wrapfigure}[9]{r}{0.4\textwidth}
\centering
\includegraphics[width=\linewidth]{img/04_erstellung-poc/jaeger_architecture-v1_edited.png}
\caption{Architektur von Jaeger, angepasst \cite{JaegerArchitecture}}
\label{fig:jaeger_architecture}
\end{wrapfigure}
	
	Um die Tracingdaten zu beleuchten und so Einblick in den architektonischen Verlauf eines Aufrufs zu erhalten, wird Jaeger verwendet. Jaeger bietet jene Tracing-Visualisierungen, die Splunk fehlen und ist zudem darauf spezialisiert Tracingdaten zu speichern und zu analysieren. Um diese Funktionen anzubieten und eine schnelle Datenabfrage zu gewährleisten, besitzt Jaeger eine darauf konzipierte Infrastruktur (vgl. \autoref{fig:jaeger_architecture}). Die verwendete Datenbank ist Cassandra und für Analyse- und Verarbeitungszwecke wird Apache Spark benutzt.
	
	Die geplante Architektur ist in \autoref{fig:architektur-technologien} zu betrachten.
	
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\linewidth]{img/04_erstellung-poc/konzept-technologien.png}
	\caption{Architektur mit speziellen Technologien}
	\label{fig:architektur-technologien}
\end{figure}

	\subsection{Übertragbarkeit}
	
	Übertragbarkeit beschäftigt sich mit der Eigenschaft eines Ansatzes oder Konzeptes auch für andere Softwareprojekte anwendbar zu sein. Grundlegend muss genannt werden, dass das Konzept sich auf Softwareprojekte, die JavaScript-basierte Webanwendungen enthalten, beschränkt. Somit wird auch die Übertragbarkeit auch nur für ähnliche Softwareprojekte bewertet.
	
	Da das Konzept nicht eine Basisstruktur darstellt, auf das man ein Projekt basieren kann, sondern viel eher ein bestehendes Projekt erweitert, lässt sich das Konzept bei reellen Projekten in verschiedensten Phasen integrieren. Weiterhin wurden die Grundkonzepte, wie die Erhebung von Tracingdaten, nicht auf etablierte Technologien basiert, sondern viel eher auf die zugrundeliegenden Disziplinen. Deswegen sind alle vorgeschlagenen Technologien und Partnersysteme austauschbar, um verschiedene Projekt- und Unternehmensvorgaben zu erfüllen. Hinzukommend ist das Konzept modular zu verstehen, denn einzelne Disziplinen werden von spezialisierten Systemen gehandelt und sind somit auch entfernbar. Diese Flexibilität der Auswahl an Komponenten und der Zusammensetzung verspricht Projektteams eine reell umsetzbare Lösung.

	Ein identifizierbares Manko ist jedoch die Menge an Partnersystemen, die die Lösung vorschlägt. Dies kann aus Wartungs- oder Kostengründen nicht für jedes Projektteam attraktiv sein, jedoch konnte leider keine weitere Verringerung der Partnersysteme erzielt werden. Wenn man aber auf einer der Datenkategorien verzichten kann, dann ließe sich ggf. auch ein Partnersystem entfernen. Sollte aber eine Technologie existieren oder sich über Zeit entwickeln, welche mehrere Disziplinen löst, so kann die Anzahl ebenso verringert werden. Dabei anzumerken ist aber auch, dass damit eine gewisse Flexibilität verloren geht. Es lässt sich aber nicht definitiv eine Bewertung hierzu erstellen, da eben solch eine Technologie nicht gefunden werden k1onnte.
	
	Alles in Allem, sollte das vorgeschlagene Konzept aber für den Großteil von Softwareprojekten in der Frontendentwicklung adaptierbar sein. Eine tiefergehende Betrachtung der Übertragbarkeit erfolgt in \autoref{sec:uebertragbarkeit}, auf Basis der tatsächlichen Implementierung. Diese Übertragbarkeit wird wahrscheinlich nicht gleich bleiben, denn die Implementierung wird bspw. einige spezifische Entscheidungen beinhalten, die eine Übertragbarkeit behindern könnten.

\section{Implementierung}

	\textit{Auf Basis des Konzeptes soll nun eine Implementierung erfolgen.}